{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T8ykH0T_eT8"
      },
      "outputs": [],
      "source": [
        "# You need to activate the GPU before running this session\n",
        "# Edit > Notebook settings or Runtime>Change runtime type and select GPU as Hardware accelerator.\n",
        "# This will run your GPU instead of your RAM\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "DATADIR = \"/content/drive/MyDrive/Sucellus Dataset/Train\"\n",
        "CATEGORIES = ['Fire',\"Neutral\",\"Smoke\"]\n",
        "\n",
        "# Firstly we must upload the data to system so that we can resize it\n",
        "\n",
        "for category in CATEGORIES:\n",
        "    path = os.path.join(DATADIR,category)\n",
        "    for img in os.listdir(path):\n",
        "          bgr_array = cv2.imread(os.path.join(path,img)) #OpenCv takes the parameters as BGR (Blue Green Red)\n",
        "          img_array = bgr_array[:, :, [2, 1, 0]]  #Thats why we need to convert this values to have RGB\n",
        "          break\n",
        "    break\n",
        "\n",
        "# If we had to work with multiple projects at same GPU we had to use this code\n",
        "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.33)\n",
        "#sess = tf.Session(config = tf.ConfigProto(gpu_options = ))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping the image\n",
        "\n",
        "IMG_SIZE =  256\n",
        "new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
        "plt.imshow(new_array)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9pKokHKsZ8dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating train data\n",
        "\n",
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "  for category in CATEGORIES:\n",
        "    path = os.path.join(DATADIR,category)\n",
        "    class_num = CATEGORIES.index(category) # 0 = fire , 1 = other , 2 = other\n",
        "    for img in os.listdir(path):\n",
        "      try:\n",
        "           bgr_array = cv2.imread(os.path.join(path,img))\n",
        "           img_array = bgr_array[:, :, [2, 1, 0]]\n",
        "           new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
        "           training_data.append([new_array,class_num])\n",
        "      except Exception as e:\n",
        "        pass\n",
        "\n",
        "create_training_data()"
      ],
      "metadata": {
        "id": "Qq9Heo63Z1Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding out our usable data number\n",
        "\n",
        "print(len(training_data))"
      ],
      "metadata": {
        "id": "IyrzwBBJbKMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffling the data for better learning\n",
        "\n",
        "import random\n",
        "random.shuffle(training_data)"
      ],
      "metadata": {
        "id": "8KAulWcYVsmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in training_data[:3]:\n",
        "  #plt.imshow(sample[0])    normally we can see the images by this\n",
        "  #plt.show()               but since the compiling time is important we can see like this\n",
        "  print(sample[1])"
      ],
      "metadata": {
        "id": "pbHiBeegdEN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing feature and label array\n",
        "\n",
        "X = []\n",
        "y = []"
      ],
      "metadata": {
        "id": "4RpReel9bzeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting and reshaping data\n",
        "for features, label in training_data:\n",
        "  X.append(features)\n",
        "  y.append(label)\n",
        "\n",
        "X = np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,3)"
      ],
      "metadata": {
        "id": "dnDvZ-VEb4YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving data to use in project\n",
        "import pickle\n",
        "pickle_out = open(\"X.pickle\",\"wb\")\n",
        "pickle.dump (X,pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"y.pickle\",\"wb\")\n",
        "pickle.dump (y,pickle_out)\n",
        "pickle_out.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "31aN4_A1cmm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading data\n",
        "import pickle\n",
        "X = pickle.load(open(\"X.pickle\",\"rb\"))\n",
        "y = pickle.load(open(\"y.pickle\",\"rb\"))\n",
        "\n",
        "X = X/255.0"
      ],
      "metadata": {
        "id": "xb3VyOhihdaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Activation, BatchNormalization\n",
        "from tensorflow.keras.layers import Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras import regularizers\n",
        "import time\n",
        "\n",
        "# Saving the model\n",
        "NAME = \"sucellus{}\".format(int(time.time()))\n",
        "tensorboard = TensorBoard(log_dir = 'logs/{}'.format(NAME))\n",
        "\n",
        "# Building the model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(128,(3,3),input_shape = X.shape[1:]))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64,(3,3),input_shape = X.shape[1:]))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128,kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(64,kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Dense(3))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
        "              optimizer ='adam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X,\n",
        "          y=np.array(y),\n",
        "          batch_size = 64,\n",
        "          steps_per_epoch=16,\n",
        "          epochs = 20,\n",
        "          validation_split = 0.15,\n",
        "          callbacks =[tensorboard] )"
      ],
      "metadata": {
        "id": "q6XCYbzwdhgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model\n",
        "model.save(NAME)\n",
        "new_model = tf.keras.models.load_model(NAME)"
      ],
      "metadata": {
        "id": "vXIDZQJPjKcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the prediction\n",
        "CATEGORIES = ['Fire','Smoke','Neutral']\n",
        "def prepare(filepath):\n",
        "  IMG_SIZE = 256\n",
        "  bgr_array = cv2.imread(os.path.join(path,img))\n",
        "  img_array = bgr_array[:, :, [2, 1, 0]]\n",
        "  new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
        "  return new_array.reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
        "model = tf.keras.models.load_model(NAME)"
      ],
      "metadata": {
        "id": "eKn26TyEkOp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict([prepare('fire.jpg')])\n",
        "print(CATEGORIES[int(prediction[0][0])])"
      ],
      "metadata": {
        "id": "gAWZhG1XMNDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict([prepare('/content/drive/MyDrive/Sucellus Dataset/Train/Neutral/image_10.jpg')])\n",
        "print(CATEGORIES[int(prediction[0][0])])"
      ],
      "metadata": {
        "id": "Qzmiqu1Pkl8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}